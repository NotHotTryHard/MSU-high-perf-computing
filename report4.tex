\documentclass[12pt,a4paper]{article}

% --- Русская типографика и математика ---
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\geometry{left=25mm,right=15mm,top=20mm,bottom=20mm}
\usepackage{microtype}
\usepackage{float} 
% --- Графика, таблицы, ссылки ---
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\sisetup{output-decimal-marker={,},round-mode=places,round-precision=3}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue}

% --- Пакеты для графиков ---
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% --- Листинги ---
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  columns=fullflexible,
  tabsize=2,
  showstringspaces=false
}

% --- Удобные обозначения ---
\newcommand{\RR}{\mathbb{R}}
\newcommand{\grad}{\nabla}
\newcommand{\lap}{\Delta}
\newcommand{\dd}{\,\mathrm{d}}

\begin{document}
\thispagestyle{empty}

\begin{center}
\vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\
{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного анализа

\vfill

{\LARGE Отчет по заданию}

\vspace{1cm}

{\Huge\bfseries Реализация параллельного алгоритма с использованием технологий MPI+CUDA}
\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Студент 616 группы}\\
  М.~Н.~Преображенский\\

  \vspace{5mm}

\end{flushright}

\vfill

\begin{center}
{\Large 7 декабря 2025}
\end{center}
\newpage
\tableofcontents
\newpage

% ====== 1. Введение ======
\section{Введение}
Цель работы — решить двумерную задачу Дирихле для уравнения Пуассона в криволинейной области методом фиктивных областей, реализовать вариант распаралеливания с использованием MPI+CUDA, после чего сравнить его с MPI-вариантом на IBM~Polus.

% ====== 2. Математическая постановка задачи ======
\section{Математическая постановка задачи}\label{sec:math}
Рассматривается задача Пуассона в криволинейной области $D\subset\RR^2$, ограниченной контуром $\gamma$:
\begin{equation}\label{eq:poisson}
  -\lap u = f(x,y), \quad (x,y)\in D,
\end{equation}
с граничным условием Дирихле первого рода
\begin{equation}\label{eq:dirichlet}
  u(x,y)=0, \quad (x,y)\in\gamma.
\end{equation}
В данной работе $f(x,y)\equiv1$. Для \textbf{варианта 10} область $D$ задаётся неравенствами
\[
  D=\{(x,y):\ x^2-4y^2>1,\ 1<x<3\},
\]
то есть область ограничена дугой гиперболы и отрезком прямой $x=3$.

% ====== 3. Численный метод ======
\section{Краткое описание численного метода решения}\label{sec:numerics}

\subsection{Метод фиктивных областей}
Пусть $D\subset\Pi=\{(x,y): A_1<x<B_1,\ A_2<y<B_2\}$ — охватывающий прямоугольник, $\widehat D=\Pi\setminus D$ — фиктивная область. В~$\Pi$ решается задача
\begin{equation}\label{eq:mfoPDE}
 -\frac{\partial}{\partial x}\!\Big(k\,u_x\Big) - \frac{\partial}{\partial y}\!\Big(k\,u_y\Big)=F(x,y),\quad (x,y)\in \Pi\setminus\gamma,\qquad u|_{\partial\Pi}=0,
\end{equation}
где кусочно-постоянный коэффициент
\[
k(x,y)=
\begin{cases}
1, & (x,y)\in D,\\
1/\varepsilon, & (x,y)\in \widehat D,
\end{cases}
\qquad \varepsilon=\max(h_x,h_y)^2.
\]

\subsection{Разностная схема}
Покроем $\Pi$ равномерной сеткой $\omega_h$ с внутренними узлами $M\times N$, шаги $h_x=\frac{B_1-A_1}{M+1}$, $h_y=\frac{B_2-A_2}{N+1}$. Дифференциальный оператор аппроксимируем пятиточечным шаблоном:
\begin{align}\label{eq:divscheme}
 -\frac{1}{h_x}\!\left(a_{i+1,j}\,\frac{w_{i+1,j}-w_{i,j}}{h_x}-a_{i,j}\,\frac{w_{i,j}-w_{i-1,j}}{h_x}\right)
 -\frac{1}{h_y}\!\left(b_{i,j+1}\,\frac{w_{i,j+1}-w_{i,j}}{h_y}-b_{i,j}\,\frac{w_{i,j}-w_{i,j-1}}{h_y}\right)
 = F_{ij},
\end{align}
где \emph{гранные коэффициенты} вычисляются аналитически через длины пересечений граней с областью $D$.

\subsection{Итерационный метод}
Для решения результирующей СЛАУ $Aw=F$ применяется \textbf{предобусловленный метод сопряжённых градиентов (PCG)} с диагональным предобуславливателем Якоби. Критерий остановки: $\|r\|/\|b\|\le10^{-8}$ или достижение максимального числа итераций.

% ====== 5. MPI реализация ======
\section{MPI-реализация}\label{sec:mpi}

\subsection{Описание реализации}
В MPI-версии используется декомпозиция области по строкам сетки:
\begin{itemize}
  \item \textbf{Распределение данных} — строки равномерно распределяются между процессами.
  \item \textbf{Halo-обмены} — граничные строки обмениваются между соседними процессами с использованием \texttt{MPI\_Isend}/\texttt{MPI\_Irecv}.
  \item \textbf{Глобальные редукции} — скалярные произведения вычисляются локально, затем суммируются через \texttt{MPI\_Allreduce}.
  \item \textbf{Сбор решения} — результат собирается на процессе 0 через \texttt{MPI\_Gatherv}.
\end{itemize}

\subsection{Результаты тестирования MPI}
\label{sec:mpi_results}
\paragraph{Референсные результаты MPI (IBM~Polus).}
В рамках данного отчёта MPI-реализация не тестировалась заново: для сравнения с MPI+CUDA ниже приведены две таблицы сильного масштабирования MPI на IBM~Polus из отдельного отчёта по MPI.

\begin{table}[H]
\centering
\caption{MPI (IBM~Polus): strong scaling на сетке $400{\times}600$ (референс из отчёта по MPI).}
\label{tab:mpi_polus_400x600}
\begin{tabular}{@{}rcccc@{}}
\toprule
$p$ & Итерации & Время $T_p$, c & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 1520 & 9.475 & 1.000 & 1.000 \\
2  & 1520 & 4.740 & 2.000 & 1.000 \\
4  & 1520 & 3.544 & 2.674 & 0.668 \\
8  & 1520 & 2.388 & 3.970 & 0.496 \\
16 & 1520 & 1.413 & 6.710 & 0.419 \\
32 & 1520 & 1.270 & 7.460 & 0.233 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MPI (IBM~Polus): strong scaling на сетке $800{\times}1200$ (референс из отчёта по MPI).}
\label{tab:mpi_polus_800x1200}
\begin{tabular}{@{}rcccc@{}}
\toprule
$p$ & Итерации & Время $T_p$, c & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 3076 & 87.900 & 1.000 & 1.000 \\
4  & 3076 & 22.404 & 3.924 & 0.981 \\
8  & 3076 & 18.915 & 4.647 & 0.581 \\
16 & 3076 & 16.916 & 5.195 & 0.188 \\
32 & 3076 & 14.616 & 6.015 & 0.188 \\
\bottomrule
\end{tabular}
\end{table}

% ====== 6. MPI+CUDA реализация ======
\section{MPI+CUDA-реализация}\label{sec:cuda}

\subsection{Архитектура гибридного решения}
В гибридной реализации MPI+CUDA вычислительно затратные операции PCG переносятся на GPU, а MPI используется для декомпозиции по строкам и обмена halo-данными между MPI-процессами. Эксперименты в настоящем отчёте выполнялись на IBM~Polus (узел с 2$\times$GPU Tesla P100). Основные вычислительные операции выполняются на GPU:

\begin{itemize}
  \item \textbf{Матрично-векторное произведение} — CUDA-ядро \texttt{matvec\_kernel} выполняет пятиточечный шаблон в параллельном режиме.
  \item \textbf{Векторные операции} (axpy, dot) — специализированные CUDA-ядра для векторной арифметики.
  \item \textbf{Предобуславливание} — применение диагонального предобуславливателя выполняется на GPU.
\end{itemize}

\subsection{Оптимизация памяти и вычислений}
\begin{itemize}
  \item Основные рабочие вектора PCG (\texttt{u}, \texttt{r}, \texttt{z}, \texttt{p}, \texttt{Ap}) и коэффициенты схемы хранятся в \textbf{глобальной памяти GPU} на протяжении всего итерационного процесса.
  \item Применяется \textbf{coalesced memory access} — потоки в варпе обращаются к последовательным элементам массива.
\end{itemize}

\subsection{Схема обмена данными}
\begin{enumerate}
  \item \textbf{Инициализация}: Копирование коэффициентов $a_{i,j}$, $b_{i,j}$, правой части $F$ с CPU на GPU.
  \item \textbf{Итерационный процесс}: 
  \begin{itemize}
    \item Вычисления на GPU (matvec, dot, axpy/обновления векторов).
    \item Halo-обмен: копирование только граничных строк (\(N\) элементов) GPU→CPU, двусторонний обмен с соседями (например, \texttt{MPI\_Sendrecv}), затем CPU→GPU.
    \item Глобальные редукции: скалярный результат локального dot-продукта копируется на CPU (одно число), выполняется \texttt{MPI\_Allreduce} и полученное скалярное значение используется в дальнейших вычислениях на GPU.
  \end{itemize}
  \item \textbf{Финализация}: Однократное копирование решения GPU→CPU для записи/сбора.
\end{enumerate}

\subsection{Времена операций MPI+CUDA}
\label{sec:cuda_timing_detail}
\paragraph{Методика измерений.}
Ниже приведены медианные времена (в секундах) по логам запусков на IBM~Polus.
Для каждой комбинации \((M{\times}N, p)\) замерялось общее время решения \texttt{solve\_time}, а также времена компонент:
\texttt{matvec\_kernel\_time}, \texttt{vector\_kernels\_time},
\texttt{halo\_mpi\_comm\_time}, \texttt{h2d\_memcpy\_time}, \texttt{d2h\_memcpy\_time}. Для каждого измерения было сделано по 5 запусков на полюсе, после чего в качестве значения была выбрана медиана (для большей робастности значений).

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): общее время решения $T$ (\texttt{solve\_time}).}
\label{tab:cuda_pivot_solve_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.00359066 & 0.0163082 & 0.0227274 & 0.0293466 & 0.019759 \\
$20{\times}20$ & 0.00774141 & 0.0228635 & 0.0463394 & 0.0452795 & 0.0489816 \\
$40{\times}40$ & 0.0156309 & 0.0439223 & 0.113014 & 0.0889949 & 0.0820525 \\
$80{\times}80$ & 0.0428222 & 0.104382 & 0.180851 & 0.187983 & 0.202463 \\
$400{\times}600$ & 0.329696 & 0.652623 & 1.10697 & 1.09778 & 0.904298 \\
$800{\times}1200$ & 1.44811 & 1.80995 & 2.37479 & 2.25902 & 2.15823 \\
$4000{\times}4000$ & 70.9051 & 40.6732 & 21.3726 & 13.904 & 10.032 \\
$6000{\times}6000$ & 238.83 & 121.049 & 67.6555 & 43.3966 & 24.9847 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{solve\_time}.}
При фиксированном \(p\) время решения растёт с увеличением размера сетки, что соответствует росту объёма вычислений.
При увеличении \(p\) поведение зависит от размера задачи: на малых сетках доминируют накладные расходы, поэтому ускорение отсутствует, а на крупных сетках появляется существенный выигрыш за счёт распараллеливания.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): время CUDA-ядра matvec (\texttt{matvec\_kernel\_time}).}
\label{tab:cuda_pivot_matvec_kernel_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.000303604 & 0.000906097 & 0.00199994 & 0.00234532 & 0.00149359 \\
$20{\times}20$ & 0.000669591 & 0.00134527 & 0.00401472 & 0.0036253 & 0.00366278 \\
$40{\times}40$ & 0.00140454 & 0.00261212 & 0.00982581 & 0.00710142 & 0.00644545 \\
$80{\times}80$ & 0.00332517 & 0.00620444 & 0.0161851 & 0.0150914 & 0.0144751 \\
$400{\times}600$ & 0.0518872 & 0.0497986 & 0.0968029 & 0.0873244 & 0.0589438 \\
$800{\times}1200$ & 0.280772 & 0.204224 & 0.205725 & 0.172604 & 0.150644 \\
$4000{\times}4000$ & 18.2407 & 9.41007 & 5.05484 & 3.16349 & 1.60991 \\
$6000{\times}6000$ & 62.0966 & 31.0678 & 16.9123 & 10.6401 & 5.70947 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{matvec\_kernel\_time}.}
На крупных сетках матвекторное произведение является одной из доминирующих вычислительных компонент и хорошо уменьшается при росте \(p\), что указывает на эффективное распараллеливание вычислений внутри matvec.
На малых сетках абсолютные времена малы, и вклад matvec меньше влияет на итоговый \texttt{solve\_time}.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): время векторных CUDA-ядер (\texttt{vector\_kernels\_time}).}
\label{tab:cuda_pivot_vector_kernels_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.00199687 & 0.00664455 & 0.0150779 & 0.0176241 & 0.0110997 \\
$20{\times}20$ & 0.00435257 & 0.0099999 & 0.0304668 & 0.0275795 & 0.0280053 \\
$40{\times}40$ & 0.00895826 & 0.0192172 & 0.0753923 & 0.0554936 & 0.0462332 \\
$80{\times}80$ & 0.0214455 & 0.0455119 & 0.119196 & 0.115417 & 0.112864 \\
$400{\times}600$ & 0.20445 & 0.282754 & 0.724616 & 0.668526 & 0.459663 \\
$800{\times}1200$ & 1.01527 & 0.872058 & 1.55077 & 1.35077 & 1.1664 \\
$4000{\times}4000$ & 52.1505 & 27.7689 & 14.7986 & 9.38014 & 5.01875 \\
$6000{\times}6000$ & 175.188 & 88.0207 & 48.1442 & 30.6104 & 16.6566 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{vector\_kernels\_time}.}
Векторные операции (axpy, dot и т.п.) в сумме занимают значительную долю времени на крупных задачах и также заметно ускоряются при росте \(p\).
На средних сетках (например, $800{\times}1200$) эта компонента остаётся доминирующей и определяет итоговую скорость решения.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): время halo-обменов MPI (\texttt{halo\_mpi\_comm\_time}).}
\label{tab:cuda_pivot_halo_mpi_comm_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0 & 0.00115545 & 0.00264152 & 0.00317932 & 0.00475239 \\
$20{\times}20$ & 0 & 0.000740277 & 0.00598937 & 0.00739346 & 0.0119843 \\
$40{\times}40$ & 0 & 0.00158236 & 0.0129917 & 0.0149169 & 0.0189365 \\
$80{\times}80$ & 0 & 0.00406047 & 0.0233639 & 0.0310438 & 0.0437344 \\
$400{\times}600$ & 0 & 0.0422832 & 0.153146 & 0.15535 & 0.220084 \\
$800{\times}1200$ & 0 & 0.0964529 & 0.31322 & 0.405732 & 0.379544 \\
$4000{\times}4000$ & 0 & 0.618848 & 1.53037 & 0.472315 & 1.95143 \\
$6000{\times}6000$ & 0 & 0.558648 & 7.01379 & 3.40072 & 3.1732 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{halo\_mpi\_comm\_time}.}
Halo-обмены практически отсутствуют при \(p=1\) и растут при увеличении \(p\).
Для крупных сеток абсолютные времена коммуникаций становятся заметными, и именно они ограничивают strong scaling на некоторых режимах.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): копирование CPU$\to$GPU (\texttt{h2d\_memcpy\_time}).}
\label{tab:cuda_pivot_h2d_memcpy_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.00003532 & 0.000825046 & 0.00154726 & 0.00286168 & 0.00346425 \\
$20{\times}20$ & 0.00006326 & 0.00100746 & 0.00250935 & 0.00402866 & 0.00496653 \\
$40{\times}40$ & 0.000133663 & 0.00185185 & 0.00538974 & 0.00725109 & 0.00670993 \\
$80{\times}80$ & 0.000150971 & 0.00437308 & 0.0084416 & 0.0155399 & 0.0142728 \\
$400{\times}600$ & 0.00214808 & 0.0275039 & 0.0494081 & 0.087946 & 0.0604123 \\
$800{\times}1200$ & 0.00751564 & 0.0698499 & 0.106886 & 0.17647 & 0.150924 \\
$4000{\times}4000$ & 0.133273 & 0.45666 & 0.446907 & 0.744892 & 0.67275 \\
$6000{\times}6000$ & 0.272633 & 0.327577 & 0.665439 & 1.09791 & 1.05151 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): копирование GPU$\to$CPU (\texttt{d2h\_memcpy\_time}).}
\label{tab:cuda_pivot_d2h_memcpy_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.000901951 & 0.00363507 & 0.00402427 & 0.00569946 & 0.00354908 \\
$20{\times}20$ & 0.001973 & 0.00596471 & 0.00809197 & 0.00884435 & 0.00853326 \\
$40{\times}40$ & 0.00375128 & 0.0111074 & 0.0196641 & 0.0177172 & 0.0146947 \\
$80{\times}80$ & 0.0125412 & 0.0263786 & 0.031432 & 0.0363773 & 0.0346245 \\
$400{\times}600$ & 0.052488 & 0.149823 & 0.192337 & 0.209621 & 0.144786 \\
$800{\times}1200$ & 0.10746 & 0.34521 & 0.411905 & 0.444341 & 0.372922 \\
$4000{\times}4000$ & 0.392142 & 1.40164 & 1.69119 & 1.87816 & 1.57321 \\
$6000{\times}6000$ & 1.11991 & 0.917503 & 2.51234 & 2.78927 & 2.57419 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ копирований CPU$\leftrightarrow$GPU.}
При \(p=1\) отсутствуют межпроцессные halo-обмены, однако остаются необходимые пересылки между CPU и GPU (инициализация данных, передача скалярных результатов глобальных редукций и финальное копирование решения).
При \(p>1\) добавляются регулярные копирования граничных строк в halo-обменах, из-за чего времена CPU$\leftrightarrow$GPU становятся заметной частью накладных расходов.
На крупных задачах копирования дают измеримый вклад и, вместе с \texttt{halo\_mpi\_comm\_time}, формируют коммуникационно-памятный оверхед гибридной схемы.

\subsection{Результаты тестирования MPI+CUDA}
\label{sec:cuda_results}
На малых задачах (до $80{\times}80$) рост числа MPI-процессов приводит к ухудшению времени решения, что объясняется доминированием накладных расходов (инициализация, синхронизации, копирования и коммуникации) над полезными вычислениями.
Для средней сетки $800{\times}1200$ в данной серии запусков минимум \texttt{solve\_time} достигается при $p=1$; при росте $p$ начинают доминировать накладные расходы на обмены и пересылки данных.
Для крупных задач $4000{\times}4000$ и $6000{\times}6000$ достигается существенное ускорение вплоть до \(S_6\approx 7.07\) и \(S_6\approx 9.56\) соответственно (по отношению к $p=1$).

% ====== 8. Анализ производительности ======
\section{Анализ эффективности и характера ускорений}\label{sec:analysis}
\subsection{Сравнение MPI и MPI+CUDA на IBM~Polus}

% ====== 9. Оценка корректности ======
\section{Оценка корректности параллельных версий}\label{sec:correctness}

\subsection{Методика верификации}
\begin{enumerate}
\item \textbf{Сходимость по сетке}: Проверка числа итераций и конечной невязки $\|r\|/\|b\|$ — все реализации дают $\|r\|/\|b\|\approx10^{-9}$ на одинаковом числе итераций.
\item \textbf{Побитовое совпадение}: Сравнение решений через среднеквадратичное отклонение:
\[
\text{RMSE}=\sqrt{\frac{1}{MN}\sum_{i,j}(u^{\text{ref}}_{ij}-u^{\text{test}}_{ij})^2} < 10^{-12}.
\]
\item \textbf{Независимость от числа процессов}: Решение при $p=1,2,3,4,6$ совпадает с точностью машинного нуля.
\end{enumerate}

% ====== 10. Заключение ======
\section{Заключение}\label{sec:concl}

\paragraph{Итог.}
В ходе работы была реализована и исследована версия алгоритма решения уравнения Пуассона методом PCG \textbf{MPI+CUDA}. Эксперименты и сравнение производительности выполнены на IBM~Polus.

\paragraph{Основные результаты (IBM~Polus).}
\begin{itemize}
\item \textbf{MPI (референс)}: strong scaling на $400{\times}600$ и $800{\times}1200$ приведён в табл.~\ref{tab:mpi_polus_400x600}, \ref{tab:mpi_polus_800x1200}; наблюдается снижение эффективности при росте числа процессов.
\item \textbf{MPI+CUDA}: для малых сеток ускорение отсутствует из-за накладных расходов, однако для крупных задач (например, $4000{\times}4000$ и $6000{\times}6000$) достигаются ускорения вплоть до \(S_6\approx 7.07\) и \(S_6\approx 9.56\) (см.~табл.~\ref{tab:cuda_pivot_solve_time}).
\end{itemize}

\paragraph{Выводы.}
GPU-ускорение эффективно проявляется на достаточно больших сетках, когда полезная работа в CUDA-ядрах доминирует над коммуникациями и копированиями.
Для малых задач MPI+CUDA нецелесообразен: накладные расходы перекрывают выигрыш от GPU.

\end{document}
