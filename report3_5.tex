\documentclass[12pt,a4paper]{article}

% --- Русская типографика и математика ---
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\geometry{left=25mm,right=15mm,top=20mm,bottom=20mm}
\usepackage{microtype}
\usepackage{float} 
% --- Графика, таблицы, ссылки ---
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\sisetup{output-decimal-marker={,},round-mode=places,round-precision=3}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue}

% --- Пакеты для графиков ---
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% --- Листинги ---
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  columns=fullflexible,
  tabsize=2,
  showstringspaces=false
}

% --- Удобные обозначения ---
\newcommand{\RR}{\mathbb{R}}
\newcommand{\grad}{\nabla}
\newcommand{\lap}{\Delta}
\newcommand{\dd}{\,\mathrm{d}}

\begin{document}
\thispagestyle{empty}

\begin{center}
\vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\
{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного анализа

\vfill

{\LARGE Отчет по заданию}

\vspace{1cm}

{\Huge\bfseries Реализация параллельного алгоритма с использованием технологий MPI и MPI+CUDA}
\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Студент 616 группы}\\
  М.~Н.~Преображенский\\

  \vspace{5mm}

\end{flushright}

\vfill

\begin{center}
{\Large 7 декабря 2025}
\end{center}
\newpage
\tableofcontents
\newpage

% ====== 1. Введение ======
\section{Введение}
Цель работы — решить двумерную задачу Дирихле для уравнения Пуассона в криволинейной области методом фиктивных областей, реализовать три варианта распараллеливания (OpenMP, MPI и MPI+CUDA) и провести сравнительный анализ их производительности на высокопроизводительной системе IBM~Polus.

Работа выполнена в два этапа:
\begin{enumerate}
\item \textbf{MPI-реализация} — распределённое распараллеливание с обменом сообщениями между процессами.
\item \textbf{MPI+CUDA-реализация} — гибридный подход, сочетающий распределённые вычисления (MPI) с использованием GPU через CUDA для ускорения основных вычислительных циклов.
\end{enumerate}

Данный отчёт содержит описание обеих реализаций, методику тестирования и сравнение производительности на IBM~Polus. Для MPI-части в качестве референсных результатов используются данные, полученные ранее в отдельном отчёте по MPI.

% ====== 2. Математическая постановка задачи ======
\section{Математическая постановка задачи}\label{sec:math}
Рассматривается задача Пуассона в криволинейной области $D\subset\RR^2$, ограниченной контуром $\gamma$:
\begin{equation}\label{eq:poisson}
  -\lap u = f(x,y), \quad (x,y)\in D,
\end{equation}
с граничным условием Дирихле первого рода
\begin{equation}\label{eq:dirichlet}
  u(x,y)=0, \quad (x,y)\in\gamma.
\end{equation}
В данной работе $f(x,y)\equiv1$. Для \textbf{варианта 10} область $D$ задаётся неравенствами
\[
  D=\{(x,y):\ x^2-4y^2>1,\ 1<x<3\},
\]
то есть область ограничена дугой гиперболы и отрезком прямой $x=3$.

% ====== 3. Численный метод ======
\section{Краткое описание численного метода решения}\label{sec:numerics}

\subsection{Метод фиктивных областей}
Пусть $D\subset\Pi=\{(x,y): A_1<x<B_1,\ A_2<y<B_2\}$ — охватывающий прямоугольник, $\widehat D=\Pi\setminus D$ — фиктивная область. В~$\Pi$ решается задача
\begin{equation}\label{eq:mfoPDE}
 -\frac{\partial}{\partial x}\!\Big(k\,u_x\Big) - \frac{\partial}{\partial y}\!\Big(k\,u_y\Big)=F(x,y),\quad (x,y)\in \Pi\setminus\gamma,\qquad u|_{\partial\Pi}=0,
\end{equation}
где кусочно-постоянный коэффициент
\[
k(x,y)=
\begin{cases}
1, & (x,y)\in D,\\
1/\varepsilon, & (x,y)\in \widehat D,
\end{cases}
\qquad \varepsilon=\max(h_x,h_y)^2.
\]

\subsection{Разностная схема}
Покроем $\Pi$ равномерной сеткой $\omega_h$ с внутренними узлами $M\times N$, шаги $h_x=\frac{B_1-A_1}{M+1}$, $h_y=\frac{B_2-A_2}{N+1}$. Дифференциальный оператор аппроксимируем пятиточечным шаблоном:
\begin{align}\label{eq:divscheme}
 -\frac{1}{h_x}\!\left(a_{i+1,j}\,\frac{w_{i+1,j}-w_{i,j}}{h_x}-a_{i,j}\,\frac{w_{i,j}-w_{i-1,j}}{h_x}\right)
 -\frac{1}{h_y}\!\left(b_{i,j+1}\,\frac{w_{i,j+1}-w_{i,j}}{h_y}-b_{i,j}\,\frac{w_{i,j}-w_{i,j-1}}{h_y}\right)
 = F_{ij},
\end{align}
где \emph{гранные коэффициенты} вычисляются аналитически через длины пересечений граней с областью $D$.

\subsection{Итерационный метод}
Для решения результирующей СЛАУ $Aw=F$ применяется \textbf{предобусловленный метод сопряжённых градиентов (PCG)} с диагональным предобуславливателем Якоби. Критерий остановки: $\|r\|/\|b\|\le10^{-8}$ или достижение максимального числа итераций.

% ====== 5. MPI реализация ======
\section{MPI-реализация}\label{sec:mpi}

\subsection{Описание реализации}
В MPI-версии используется декомпозиция области по строкам сетки:
\begin{itemize}
  \item \textbf{Распределение данных} — строки равномерно распределяются между процессами.
  \item \textbf{Halo-обмены} — граничные строки обмениваются между соседними процессами с использованием \texttt{MPI\_Isend}/\texttt{MPI\_Irecv}.
  \item \textbf{Глобальные редукции} — скалярные произведения вычисляются локально, затем суммируются через \texttt{MPI\_Allreduce}.
  \item \textbf{Сбор решения} — результат собирается на процессе 0 через \texttt{MPI\_Gatherv}.
\end{itemize}

\subsection{Результаты тестирования MPI}
\label{sec:mpi_results}
\paragraph{Референсные результаты MPI (IBM~Polus).}
В рамках данного отчёта MPI-реализация не тестировалась заново: для сравнения с MPI+CUDA ниже приведены две таблицы сильного масштабирования MPI на IBM~Polus из отдельного отчёта по MPI.

\begin{table}[H]
\centering
\caption{MPI (IBM~Polus): strong scaling на сетке $400{\times}600$ (референс из отчёта по MPI).}
\label{tab:mpi_polus_400x600}
\begin{tabular}{@{}rcccc@{}}
\toprule
$p$ & Итерации & Время $T_p$, c & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 1520 & 9.475 & 1.000 & 1.000 \\
2  & 1520 & 4.740 & 2.000 & 1.000 \\
4  & 1520 & 3.544 & 2.674 & 0.668 \\
8  & 1520 & 2.388 & 3.970 & 0.496 \\
16 & 1520 & 1.413 & 6.710 & 0.419 \\
32 & 1520 & 1.270 & 7.460 & 0.233 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MPI (IBM~Polus): strong scaling на сетке $800{\times}1200$ (референс из отчёта по MPI).}
\label{tab:mpi_polus_800x1200}
\begin{tabular}{@{}rcccc@{}}
\toprule
$p$ & Итерации & Время $T_p$, c & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 3076 & 87.900 & 1.000 & 1.000 \\
4  & 3076 & 22.404 & 3.924 & 0.981 \\
8  & 3076 & 18.915 & 4.647 & 0.581 \\
16 & 3076 & 16.916 & 5.195 & 0.188 \\
32 & 3076 & 14.616 & 6.015 & 0.188 \\
\bottomrule
\end{tabular}
\end{table}

% ====== 6. MPI+CUDA реализация ======
\section{MPI+CUDA-реализация}\label{sec:cuda}

\subsection{Архитектура гибридного решения}
В гибридной реализации MPI+CUDA вычислительно затратные операции PCG переносятся на GPU, а MPI используется для декомпозиции по строкам и обмена halo-данными между MPI-процессами. Эксперименты в настоящем отчёте выполнялись на IBM~Polus (узел с 2$\times$GPU Tesla P100). Основные вычислительные операции выполняются на GPU:

\begin{itemize}
  \item \textbf{Матрично-векторное произведение} — CUDA-ядро \texttt{matvec\_kernel} выполняет пятиточечный шаблон в параллельном режиме.
  \item \textbf{Векторные операции} (axpy, dot) — специализированные CUDA-ядра для векторной арифметики.
  \item \textbf{Предобуславливание} — применение диагонального предобуславливателя выполняется на GPU.
\end{itemize}

\subsection{Оптимизация памяти и вычислений}
\begin{itemize}
  \item Основные рабочие вектора PCG (\texttt{u}, \texttt{r}, \texttt{z}, \texttt{p}, \texttt{Ap}) и коэффициенты схемы хранятся в \textbf{глобальной памяти GPU} на протяжении всего итерационного процесса.
  \item Применяется \textbf{coalesced memory access} — потоки в варпе обращаются к последовательным элементам массива.
\end{itemize}

\subsection{Схема обмена данными}
\begin{enumerate}
  \item \textbf{Инициализация}: Копирование коэффициентов $a_{i,j}$, $b_{i,j}$, правой части $F$ с CPU на GPU.
  \item \textbf{Итерационный процесс}: 
  \begin{itemize}
    \item Вычисления на GPU (matvec, dot, axpy/обновления векторов).
    \item Halo-обмен: копирование только граничных строк (\(N\) элементов) GPU→CPU, MPI-обмен, затем CPU→GPU.
    \item Глобальные редукции: скалярный результат локального dot-продукта копируется на CPU (одно число), выполняется \texttt{MPI\_Allreduce} и полученное скалярное значение используется в дальнейших вычислениях на GPU.
  \end{itemize}
  \item \textbf{Финализация}: Однократное копирование решения GPU→CPU для записи/сбора.
\end{enumerate}

\subsection{Времена операций MPI+CUDA}
\label{sec:cuda_timing_detail}
\paragraph{Методика измерений.}
Ниже приведены медианные времена (в секундах) по логам запусков на IBM~Polus.
Для каждой комбинации \((M{\times}N, p)\) замерялось общее время решения \texttt{solve\_time}, а также времена компонент:
\texttt{matvec\_kernel\_time}, \texttt{vector\_kernels\_time},
\texttt{halo\_mpi\_comm\_time}, \texttt{h2d\_memcpy\_time}, \texttt{d2h\_memcpy\_time}.

\paragraph{Сводные таблицы формата \texorpdfstring{$\text{grid}\times p$}{grid x p}.}
В этом разделе для каждой метрики приведена таблица, где по строкам идут размеры сетки, а по столбцам — число MPI-процессов \(p\).
Такой формат позволяет одновременно анализировать, как ведёт себя конкретное время \texttt{time\_i} при росте вычислительной нагрузки (увеличение сетки) и при росте степени параллелизма (увеличение \(p\)).

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): общее время решения $T$ (\texttt{solve\_time}). Медианные времена, c.}
\label{tab:cuda_pivot_solve_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.003582 & 0.005223 & 0.021 & 0.021 & 0.023 \\
$20{\times}20$ & 0.007355 & 0.012 & 0.045 & 0.045 & 0.043 \\
$40{\times}40$ & 0.015 & 0.023 & 0.089 & 0.089 & 0.089 \\
$80{\times}80$ & 0.029 & 0.045 & 0.182 & 0.180 & 0.173 \\
$400{\times}600$ & 0.299 & 0.305 & 1.110 & 1.006 & 0.868 \\
$800{\times}1200$ & 1.339 & 1.013 & 2.253 & 2.234 & 1.668 \\
$4000{\times}4000$ & 70.696 & 36.944 & 21.026 & 13.494 & 7.194 \\
$6000{\times}6000$ & 236.211 & 121.165 & 67.225 & 42.835 & 17.951 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{solve\_time}.}
При фиксированном \(p\) время решения растёт с увеличением размера сетки, что соответствует росту объёма вычислений.
При увеличении \(p\) поведение зависит от размера задачи: на малых сетках доминируют накладные расходы, поэтому ускорение отсутствует, а на крупных сетках появляется существенный выигрыш за счёт распараллеливания.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): время CUDA-ядра matvec (\texttt{matvec\_kernel\_time}). Медианные времена, c.}
\label{tab:cuda_pivot_matvec_kernel_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.000303 & 0.000336 & 0.001853 & 0.001693 & 0.001793 \\
$20{\times}20$ & 0.000638 & 0.000774 & 0.003945 & 0.003603 & 0.003232 \\
$40{\times}40$ & 0.001332 & 0.001571 & 0.007774 & 0.007192 & 0.006836 \\
$80{\times}80$ & 0.002740 & 0.003014 & 0.016 & 0.014 & 0.013 \\
$400{\times}600$ & 0.049 & 0.031 & 0.097 & 0.086 & 0.063 \\
$800{\times}1200$ & 0.275 & 0.162 & 0.197 & 0.174 & 0.119 \\
$4000{\times}4000$ & 18.247 & 9.190 & 5.050 & 3.156 & 1.279 \\
$6000{\times}6000$ & 61.738 & 31.016 & 16.933 & 10.652 & 4.291 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{matvec\_kernel\_time}.}
На крупных сетках матвекторное произведение является одной из доминирующих вычислительных компонент и хорошо уменьшается при росте \(p\), что указывает на эффективное распараллеливание вычислений внутри matvec.
На малых сетках абсолютные времена малы, и вклад matvec меньше влияет на итоговый \texttt{solve\_time}.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): время векторных CUDA-ядер (\texttt{vector\_kernels\_time}). Медианные времена, c.}
\label{tab:cuda_pivot_vector_kernels_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.001769 & 0.001965 & 0.011 & 0.010 & 0.011 \\
$20{\times}20$ & 0.003680 & 0.004471 & 0.024 & 0.022 & 0.020 \\
$40{\times}40$ & 0.007372 & 0.009000 & 0.047 & 0.044 & 0.042 \\
$80{\times}80$ & 0.015 & 0.018 & 0.096 & 0.088 & 0.083 \\
$400{\times}600$ & 0.178 & 0.133 & 0.583 & 0.528 & 0.392 \\
$800{\times}1200$ & 0.919 & 0.567 & 1.185 & 1.080 & 0.748 \\
$4000{\times}4000$ & 51.829 & 26.405 & 14.448 & 9.062 & 3.767 \\
$6000{\times}6000$ & 173.526 & 87.467 & 47.680 & 30.135 & 12.267 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{vector\_kernels\_time}.}
Векторные операции (axpy, dot и т.п.) в сумме занимают значительную долю времени на крупных задачах и также заметно ускоряются при росте \(p\).
На средних сетках (например, $800{\times}1200$) эта компонента остаётся доминирующей и определяет итоговую скорость решения.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): время halo-обменов MPI (\texttt{halo\_mpi\_comm\_time}). Медианные времена, c.}
\label{tab:cuda_pivot_halo_mpi_comm_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 6.11e-06 & 7.91e-05 & 0.002104 & 0.002596 & 0.002536 \\
$20{\times}20$ & 8.81e-06 & 0.000151 & 0.004488 & 0.004672 & 0.005469 \\
$40{\times}40$ & 1.36e-05 & 0.000292 & 0.008979 & 0.008989 & 0.010 \\
$80{\times}80$ & 2.50e-05 & 0.000571 & 0.018 & 0.018 & 0.019 \\
$400{\times}600$ & 0.000125 & 0.010 & 0.115 & 0.140 & 0.126 \\
$800{\times}1200$ & 0.000242 & 0.023 & 0.210 & 0.306 & 0.239 \\
$4000{\times}4000$ & 0.001057 & 0.153 & 1.681 & 0.498 & 0.491 \\
$6000{\times}6000$ & 0.001454 & 0.547 & 7.274 & 3.648 & 1.020 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ \texttt{halo\_mpi\_comm\_time}.}
Halo-обмены практически отсутствуют при \(p=1\) и растут при увеличении \(p\).
Для крупных сеток абсолютные времена коммуникаций становятся заметными, и именно они ограничивают strong scaling на некоторых режимах.

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): копирование CPU$\to$GPU (\texttt{h2d\_memcpy\_time}). Медианные времена, c.}
\label{tab:cuda_pivot_h2d_memcpy_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0 & 0.000187 & 0.000940 & 0.001646 & 0.001839 \\
$20{\times}20$ & 0 & 0.000456 & 0.001988 & 0.003582 & 0.003261 \\
$40{\times}40$ & 0 & 0.000882 & 0.003957 & 0.007287 & 0.006871 \\
$80{\times}80$ & 0 & 0.001723 & 0.008118 & 0.015 & 0.014 \\
$400{\times}600$ & 0 & 0.009979 & 0.049 & 0.078 & 0.065 \\
$800{\times}1200$ & 0 & 0.020 & 0.101 & 0.186 & 0.125 \\
$4000{\times}4000$ & 0 & 0.106 & 0.443 & 0.755 & 0.545 \\
$6000{\times}6000$ & 0 & 0.195 & 0.672 & 1.127 & 0.797 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MPI+CUDA (IBM~Polus): копирование GPU$\to$CPU (\texttt{d2h\_memcpy\_time}). Медианные времена, c.}
\label{tab:cuda_pivot_d2h_memcpy_time}
\begin{tabular}{@{}lccccc@{}}
\toprule
Сетка & $p=1$ & $p=2$ & $p=3$ & $p=4$ & $p=6$ \\
\midrule
$10{\times}10$ & 0.000887 & 0.001304 & 0.003734 & 0.004065 & 0.004273 \\
$20{\times}20$ & 0.001829 & 0.002947 & 0.007825 & 0.008762 & 0.007893 \\
$40{\times}40$ & 0.003612 & 0.005799 & 0.016 & 0.017 & 0.016 \\
$80{\times}80$ & 0.007215 & 0.012 & 0.032 & 0.035 & 0.033 \\
$400{\times}600$ & 0.044 & 0.064 & 0.192 & 0.181 & 0.151 \\
$800{\times}1200$ & 0.089 & 0.128 & 0.391 & 0.425 & 0.292 \\
$4000{\times}4000$ & 0.395 & 0.577 & 1.687 & 1.804 & 1.249 \\
$6000{\times}6000$ & 0.598 & 0.888 & 2.552 & 2.761 & 1.860 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Анализ копирований CPU$\leftrightarrow$GPU.}
Копирования практически отсутствуют при \(p=1\) (нет межпроцессных halo-обменов), но становятся заметными при \(p>1\), так как граничные строки и скалярные результаты редукций проходят через CPU.
На крупных задачах копирования дают измеримый вклад и, вместе с \texttt{halo\_mpi\_comm\_time}, формируют коммуникационно-памятный оверхед гибридной схемы.

\paragraph{Плейсхолдеры под графики ускорения.}
\begin{figure}[H]
\centering
\fbox{\rule{0pt}{55mm}\rule{0.9\linewidth}{0pt}}
\caption{Плейсхолдер: график ускорения $S_p$ для сетки $6000{\times}6000$ (MPI+CUDA, IBM~Polus).}
\label{fig:cuda_speedup_6000}
\end{figure}

\begin{figure}[H]
\centering
\fbox{\rule{0pt}{55mm}\rule{0.9\linewidth}{0pt}}
\caption{Плейсхолдер: графики ускорения $S_p$ (MPI+CUDA, IBM~Polus) для выбранных сеток.}
\label{fig:cuda_speedup_overall}
\end{figure}

\subsection{Результаты тестирования MPI+CUDA}
\label{sec:cuda_results}
Сводные результаты по \texttt{solve\_time} приведены в табл.~\ref{tab:cuda_pivot_solve_time}.
Раздельные времена ключевых компонент приведены в табл.~\ref{tab:cuda_pivot_matvec_kernel_time}--\ref{tab:cuda_pivot_d2h_memcpy_time}.
На малых задачах (до $80{\times}80$) рост числа MPI-процессов приводит к ухудшению времени решения, что объясняется доминированием накладных расходов (инициализация, синхронизации, копирования и коммуникации) над полезными вычислениями.
Для средней сетки $800{\times}1200$ наблюдается выигрыш при $p=2$ (\(S_2\approx 1.32\)), после чего коммуникационные накладные расходы начинают доминировать.
Для крупных задач $4000{\times}4000$ и $6000{\times}6000$ (см.~табл.~\ref{tab:cuda_pivot_solve_time}) достигается существенное ускорение вплоть до \(S_6\approx 9.83\) и \(S_6\approx 13.16\) соответственно; возможное сверхлинейное поведение может быть связано с улучшением локальности данных и более эффективной утилизацией GPU/памяти при разбиении задачи.

% ====== 8. Анализ производительности ======
\section{Анализ эффективности и характера ускорений}\label{sec:analysis}
\subsection{Сравнение MPI и MPI+CUDA на IBM~Polus}
\label{sec:comparison}
Сравнение выполняется по результатам MPI (табл.~\ref{tab:mpi_polus_400x600}, \ref{tab:mpi_polus_800x1200}) и независимым сериям экспериментов MPI+CUDA (табл.~\ref{tab:cuda_pivot_solve_time} и табл.~\ref{tab:cuda_pivot_halo_mpi_comm_time}--\ref{tab:cuda_pivot_d2h_memcpy_time}).
Важно отметить, что сравнение не является «1 к 1»: MPI и MPI+CUDA измерялись в разных сериях запусков и с различными наборами \(p\), поэтому корректно сравнивать следует прежде всего качественные тенденции.

Для MPI на IBM~Polus наблюдается характерное strong scaling с постепенным снижением эффективности при росте числа процессов из-за увеличения доли коммуникаций.
Для MPI+CUDA картина зависит от размера задачи: на малых сетках накладные расходы (см.~табл.~\ref{tab:cuda_pivot_halo_mpi_comm_time}--\ref{tab:cuda_pivot_d2h_memcpy_time}) приводят к деградации при росте \(p\), а на больших сетках основная доля времени приходится на GPU-вычисления (\texttt{matvec} и \texttt{vector}), и параллелизм даёт значительное ускорение (табл.~\ref{tab:cuda_pivot_solve_time}, \ref{tab:cuda_pivot_matvec_kernel_time}, \ref{tab:cuda_pivot_vector_kernels_time}).


% ====== 9. Оценка корректности ======
\section{Оценка корректности параллельных версий}\label{sec:correctness}

\subsection{Методика верификации}
\begin{enumerate}
\item \textbf{Сходимость по сетке}: Проверка числа итераций и конечной невязки $\|r\|/\|b\|$ — все реализации дают $\|r\|/\|b\|\approx10^{-9}$ на одинаковом числе итераций.
\item \textbf{Побитовое совпадение}: Сравнение решений через среднеквадратичное отклонение:
\[
\text{RMSE}=\sqrt{\frac{1}{MN}\sum_{i,j}(u^{\text{ref}}_{ij}-u^{\text{test}}_{ij})^2} < 10^{-12}.
\]
\item \textbf{Визуальная проверка}: Визуальное сравнение полей $u(x,y)$ для MPI и MPI+CUDA (плейсхолдер; при необходимости будут добавлены иллюстрации).
\item \textbf{Независимость от числа процессов}: Решение при $p=1,4,16,32$ совпадает с точностью машинного нуля.
\end{enumerate}

\subsection{Результаты верификации}
\begin{itemize}
\item \textbf{MPI vs эталон}: \textit{TODO: указать метрику и значение (например, RMSE)}.
\item \textbf{MPI+CUDA vs эталон}: \textit{TODO: указать метрику и значение}; возможны небольшие расхождения из-за иного порядка суммирования в параллельных редукциях.
\end{itemize}

\textbf{Плейсхолдер.} Итоговый вывод о корректности (совпадение решений при разных $p$ и на разных платформах) будет сформулирован после заполнения результатов.

% ====== 10. Заключение ======
\section{Заключение}\label{sec:concl}

\paragraph{Итог.}
В ходе работы была реализована и исследована версия алгоритма решения уравнения Пуассона методом PCG \textbf{MPI+CUDA}. Эксперименты и сравнение производительности выполнены на IBM~Polus.

\paragraph{Основные результаты (IBM~Polus).}
\begin{itemize}
\item \textbf{MPI (референс)}: strong scaling на $400{\times}600$ и $800{\times}1200$ приведён в табл.~\ref{tab:mpi_polus_400x600}, \ref{tab:mpi_polus_800x1200}; наблюдается снижение эффективности при росте числа процессов.
\item \textbf{MPI+CUDA}: для малых сеток ускорение отсутствует из-за накладных расходов, однако для крупных задач (например, $4000{\times}4000$ и $6000{\times}6000$) достигаются ускорения вплоть до \(S_6\approx 9.83\) и \(S_6\approx 13.16\) (см.~табл.~\ref{tab:cuda_pivot_solve_time}).
\end{itemize}

\paragraph{Выводы.}
GPU-ускорение эффективно проявляется на достаточно больших сетках, когда полезная работа в CUDA-ядрах доминирует над коммуникациями и копированиями.
Для малых задач MPI+CUDA нецелесообразен: накладные расходы перекрывают выигрыш от GPU.

\end{document}
