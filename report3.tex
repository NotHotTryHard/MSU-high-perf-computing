\documentclass[12pt,a4paper]{article}

% --- Русская типографика и математика ---
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\geometry{left=25mm,right=15mm,top=20mm,bottom=20mm}
\usepackage{microtype}
\usepackage{float} 
% --- Графика, таблицы, ссылки ---
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\sisetup{output-decimal-marker={,},round-mode=places,round-precision=3}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue}

% --- Пакеты для графиков ---
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% --- Листинги ---
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  columns=fullflexible,
  tabsize=2,
  showstringspaces=false
}

% --- Удобные обозначения ---
\newcommand{\RR}{\mathbb{R}}
\newcommand{\grad}{\nabla}
\newcommand{\lap}{\Delta}
\newcommand{\dd}{\,\mathrm{d}}

\begin{document}
\thispagestyle{empty}

\begin{center}
\vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\
{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного анализа

\vfill

{\LARGE Отчет по заданию}

\vspace{1cm}

{\Huge\bfseries Реализация параллельного алгоритма с использованием технологий OpenMP, MPI и MPI+CUDA}
\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Студент 616 группы}\\
  М.~Н.~Преображенский\\

  \vspace{5mm}

\end{flushright}

\vfill

\begin{center}
{\Large 7 декабря 2025}
\end{center}
\newpage
\tableofcontents
\newpage

% ====== 1. Введение ======
\section{Введение}
Цель работы — решить двумерную задачу Дирихле для уравнения Пуассона в криволинейной области методом фиктивных областей, реализовать три варианта распараллеливания (OpenMP, MPI и MPI+CUDA) и провести сравнительный анализ их производительности на высокопроизводительной системе IBM~Polus.

Работа выполнена в три этапа:
\begin{enumerate}
\item \textbf{OpenMP-реализация} — распараллеливание на уровне общей памяти с использованием директив OpenMP.
\item \textbf{MPI-реализация} — распределённое распараллеливание с обменом сообщениями между процессами.
\item \textbf{MPI+CUDA-реализация} — гибридный подход, сочетающий распределённые вычисления (MPI) с использованием GPU через CUDA для ускорения основных вычислительных циклов.
\end{enumerate}

Данный отчёт содержит описание всех трёх реализаций, результаты тестирования, детальный анализ времен (инициализация, копирование данных CPU$\leftrightarrow$GPU, вычисления, коммуникации) и сравнение эффективности распараллеливания.

% ====== 2. Математическая постановка задачи ======
\section{Математическая постановка задачи}\label{sec:math}
Рассматривается задача Пуассона в криволинейной области $D\subset\RR^2$, ограниченной контуром $\gamma$:
\begin{equation}\label{eq:poisson}
  -\lap u = f(x,y), \quad (x,y)\in D,
\end{equation}
с граничным условием Дирихле первого рода
\begin{equation}\label{eq:dirichlet}
  u(x,y)=0, \quad (x,y)\in\gamma.
\end{equation}
В данной работе $f(x,y)\equiv1$. Для \textbf{варианта 10} область $D$ задаётся неравенствами
\[
  D=\{(x,y):\ x^2-4y^2>1,\ 1<x<3\},
\]
то есть область ограничена дугой гиперболы и отрезком прямой $x=3$.

% ====== 3. Численный метод ======
\section{Краткое описание численного метода решения}\label{sec:numerics}

\subsection{Метод фиктивных областей}
Пусть $D\subset\Pi=\{(x,y): A_1<x<B_1,\ A_2<y<B_2\}$ — охватывающий прямоугольник, $\widehat D=\Pi\setminus D$ — фиктивная область. В~$\Pi$ решается задача
\begin{equation}\label{eq:mfoPDE}
 -\frac{\partial}{\partial x}\!\Big(k\,u_x\Big) - \frac{\partial}{\partial y}\!\Big(k\,u_y\Big)=F(x,y),\quad (x,y)\in \Pi\setminus\gamma,\qquad u|_{\partial\Pi}=0,
\end{equation}
где кусочно-постоянный коэффициент
\[
k(x,y)=
\begin{cases}
1, & (x,y)\in D,\\
1/\varepsilon, & (x,y)\in \widehat D,
\end{cases}
\qquad \varepsilon=\max(h_x,h_y)^2.
\]

\subsection{Разностная схема}
Покроем $\Pi$ равномерной сеткой $\omega_h$ с внутренними узлами $M\times N$, шаги $h_x=\frac{B_1-A_1}{M+1}$, $h_y=\frac{B_2-A_2}{N+1}$. Дифференциальный оператор аппроксимируем пятиточечным шаблоном:
\begin{align}\label{eq:divscheme}
 -\frac{1}{h_x}\!\left(a_{i+1,j}\,\frac{w_{i+1,j}-w_{i,j}}{h_x}-a_{i,j}\,\frac{w_{i,j}-w_{i-1,j}}{h_x}\right)
 -\frac{1}{h_y}\!\left(b_{i,j+1}\,\frac{w_{i,j+1}-w_{i,j}}{h_y}-b_{i,j}\,\frac{w_{i,j}-w_{i,j-1}}{h_y}\right)
 = F_{ij},
\end{align}
где \emph{гранные коэффициенты} вычисляются аналитически через длины пересечений граней с областью $D$.

\subsection{Итерационный метод}
Для решения результирующей СЛАУ $Aw=F$ применяется \textbf{предобусловленный метод сопряжённых градиентов (PCG)} с диагональным предобуславливателем Якоби. Критерий остановки: $\|r\|/\|b\|\le10^{-8}$ или достижение максимального числа итераций.

% ====== 4. OpenMP реализация ======
\section{OpenMP-реализация}\label{sec:openmp}

\subsection{Описание реализации}
В OpenMP-версии распараллеливание выполняется на уровне общей памяти. Основные параллельные циклы:
\begin{itemize}
  \item \textbf{Инициализация коэффициентов} $a_{i,j}$ и $b_{i,j}$ с использованием директивы \texttt{\#pragma omp parallel for}.
  \item \textbf{Формирование правой части} $F_{ij}$ — параллельный цикл по всем узлам сетки.
  \item \textbf{Матрично-векторное произведение} в PCG — параллелизация основного цикла по строкам.
  \item \textbf{Векторные операции} (сложение, скалярные произведения) — распараллелены с использованием \texttt{reduction}.
\end{itemize}

\subsection{Результаты тестирования OpenMP}
Тестирование проводилось на сетке $400\times600$ с различным числом потоков.

\begin{table}[H]
\centering
\caption{Производительность OpenMP на сетке $400\times600$.}
\label{tab:openmp_400x600}
\begin{tabular}{@{}rcccc@{}}
\toprule
Потоки $p$ & Итерации & Время $T_p$, с & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 1520 & 9.475  & 1.000 & 1.000 \\
2  & 1520 & 4.740  & 2.000 & 1.000 \\
4  & 1520 & 3.544  & 2.674 & 0.668 \\
8  & 1520 & 2.388  & 3.970 & 0.496 \\
16 & 1520 & 1.413  & 6.710 & 0.419 \\
32 & 1520 & 1.270  & 7.460 & 0.233 \\
\bottomrule
\end{tabular}
\end{table}

% ====== 5. MPI реализация ======
\section{MPI-реализация}\label{sec:mpi}

\subsection{Описание реализации}
В MPI-версии используется декомпозиция области по строкам сетки:
\begin{itemize}
  \item \textbf{Распределение данных} — строки равномерно распределяются между процессами.
  \item \textbf{Halo-обмены} — граничные строки обмениваются между соседними процессами с использованием \texttt{MPI\_Isend}/\texttt{MPI\_Irecv}.
  \item \textbf{Глобальные редукции} — скалярные произведения вычисляются локально, затем суммируются через \texttt{MPI\_Allreduce}.
  \item \textbf{Сбор решения} — результат собирается на процессе 0 через \texttt{MPI\_Gatherv}.
\end{itemize}

\subsection{Результаты тестирования MPI}

\begin{table}[H]
\centering
\caption{Производительность MPI на сетке $400\times600$.}
\label{tab:mpi_400x600}
\begin{tabular}{@{}rcccc@{}}
\toprule
Процессы $p$ & Итерации & Время $T_p$, с & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 1520 & 10.054 & 1.000 & 1.000 \\
2  & 1520 & 5.819  & 1.728 & 0.864 \\
4  & 1520 & 3.277  & 3.068 & 0.767 \\
8  & 1520 & 1.943  & 5.175 & 0.647 \\
16 & 1520 & 1.311  & 7.670 & 0.479 \\
32 & 1520 & 1.129  & 8.905 & 0.278 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Производительность MPI на сетке $800\times1200$.}
\label{tab:mpi_800x1200}
\begin{tabular}{@{}rcccc@{}}
\toprule
Процессы $p$ & Итерации & Время $T_p$, с & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 3076 & 87.900 & 1.000 & 1.000 \\
4  & 3076 & 22.404 & 3.924 & 0.981 \\
8  & 3076 & 18.915 & 4.647 & 0.581 \\
16 & 3076 & 16.916 & 5.195 & 0.325 \\
32 & 3076 & 14.616 & 6.015 & 0.188 \\
\bottomrule
\end{tabular}
\end{table}

% ====== 6. MPI+CUDA реализация ======
\section{MPI+CUDA-реализация}\label{sec:cuda}

\subsection{Архитектура гибридного решения}
В гибридной реализации MPI+CUDA каждый MPI-процесс управляет одним GPU-ускорителем (Tesla P100). Основные вычислительные операции выполняются на GPU:

\begin{itemize}
  \item \textbf{Матрично-векторное произведение} — CUDA-ядро \texttt{matvec\_kernel} выполняет пятиточечный шаблон в параллельном режиме.
  \item \textbf{Векторные операции} (axpy, dot) — специализированные CUDA-ядра для векторной арифметики.
  \item \textbf{Предобуславливание} — применение диагонального предобуславливателя выполняется на GPU.
\end{itemize}

\subsection{Оптимизация памяти и вычислений}
\begin{itemize}
  \item Данные хранятся в \textbf{глобальной памяти GPU}. Использование разделяемой памяти не применялось, так как пятиточечный шаблон хорошо оптимизируется компилятором и не требует явного управления кэшем для данной задачи.
  \item Применяется \textbf{coalesced memory access} — потоки в варпе обращаются к последовательным элементам массива.
  \item \textbf{Архитектура GPU}: sm\_35 (compute capability 3.5), что соответствует Tesla K40/P100.
\end{itemize}

\subsection{Схема обмена данными}
\begin{enumerate}
  \item \textbf{Инициализация}: Копирование коэффициентов $a_{i,j}$, $b_{i,j}$, правой части $F$ с CPU на GPU.
  \item \textbf{Итерационный процесс}: 
  \begin{itemize}
    \item Вычисления на GPU (matvec, dot, axpy).
    \item Halo-обмен: копирование граничных строк GPU→CPU, MPI-обмен, CPU→GPU.
    \item Глобальные редукции: результат dot-продукта копируется на CPU, выполняется \texttt{MPI\_Allreduce}, результат возвращается на GPU.
  \end{itemize}
  \item \textbf{Финализация}: Копирование решения GPU→CPU для записи в файл.
\end{enumerate}

\subsection{Времена операций MPI+CUDA (детальный анализ)}
Для сетки $400\times600$ с $p=1$ (типичный запуск):

\begin{table}[H]
\centering
\caption{Детальная разбивка времён для MPI+CUDA ($400\times600$, $p=1$).}
\label{tab:cuda_timing_detail}
\begin{tabular}{@{}lc@{}}
\toprule
Операция & Время, с \\
\midrule
Инициализация коэффициентов (CPU) & 0.087 \\
Копирование данных CPU→GPU (начальное) & 0.012 \\
Итерационный процесс (PCG на GPU) & 10.180 \\
\quad в том числе: & \\
\quad\quad — вычисления на GPU (ядра) & 9.854 \\
\quad\quad — halo-обмены MPI (CPU$\leftrightarrow$GPU) & 0.246 \\
\quad\quad — глобальные редукции MPI & 0.080 \\
Копирование решения GPU→CPU (финал) & 0.005 \\
Сбор решения (MPI\_Gather) & 0.059 \\
\midrule
\textbf{Общее время решения} & \textbf{10.344} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Пояснение к таблице:}
\begin{itemize}
\item \textbf{Инициализация (0.087 с)} — вычисление коэффициентов $a_{i,j}$, $b_{i,j}$, правой части на CPU перед передачей на GPU.
\item \textbf{Копирование CPU→GPU (0.012 с)} — однократная передача входных данных размером $\sim$10~МБ.
\item \textbf{Вычисления на GPU (9.854 с)} — суммарное время выполнения CUDA-ядер за 1520 итераций.
\item \textbf{Halo-обмены (0.246 с)} — копирование граничных строк для MPI-обменов: GPU→CPU (перед \texttt{MPI\_Isend}) и CPU→GPU (после \texttt{MPI\_Irecv}).
\item \textbf{Глобальные редукции (0.080 с)} — суммарное время на копирование результатов скалярных произведений GPU→CPU, выполнение \texttt{MPI\_Allreduce} и возврат CPU→GPU.
\item \textbf{Копирование GPU→CPU (финал, 0.005 с)} — однократная передача решения размером $\sim$2~МБ.
\item \textbf{MPI\_Gather (0.059 с)} — сборка распределённого решения на процессе 0.
\end{itemize}

Из анализа видно, что:
\begin{enumerate}
\item Основное время ($\sim$95\%) занимают вычисления на GPU — это ожидаемо для compute-bound задачи.
\item Накладные расходы на копирование данных между CPU и GPU составляют $\sim$3\% от общего времени.
\item Коммуникационные накладные расходы MPI (halo + редукции) составляют $\sim$3\% при $p=1$ и увеличиваются с ростом числа процессов.
\end{enumerate}

\subsection{Результаты тестирования MPI+CUDA}

\begin{table}[H]
\centering
\caption{Производительность MPI+CUDA на сетке $400\times600$.}
\label{tab:cuda_400x600}
\begin{tabular}{@{}rcccc@{}}
\toprule
Процессы $p$ & Итерации & Время $T_p$, с & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 1520 & 10.344 & 1.000 & 1.000 \\
2  & 1520 & 7.759  & 1.333 & 0.666 \\
4  & 1520 & 6.357  & 1.627 & 0.407 \\
8  & 1520 & 3.430  & 3.016 & 0.377 \\
16 & 1520 & 1.351  & 7.656 & 0.478 \\
32 & 1520 & 0.953  & 10.853 & 0.339 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Производительность MPI+CUDA на сетке $800\times1200$.}
\label{tab:cuda_800x1200}
\begin{tabular}{@{}rcccc@{}}
\toprule
Процессы $p$ & Итерации & Время $T_p$, с & Ускорение $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 3012 & 85.356 & 1.000 & 1.000 \\
2  & 3012 & 49.198 & 1.735 & 0.867 \\
4  & 3012 & 25.683 & 3.323 & 0.831 \\
8  & 3012 & 16.801 & 5.081 & 0.635 \\
16 & 3012 & 10.946 & 7.799 & 0.487 \\
32 & 3012 & 7.821  & 10.913 & 0.341 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\linewidth]{3. MPI + CUDA/scaling_400x600.png}
\caption{График ускорения для сетки $400\times600$ (MPI+CUDA).}
\label{fig:cuda_400x600}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\linewidth]{3. MPI + CUDA/scaling_800x1200.png}
\caption{График ускорения для сетки $800\times1200$ (MPI+CUDA).}
\label{fig:cuda_800x1200}
\end{figure}

% ====== 7. Сравнение всех реализаций ======
\section{Сравнительный анализ всех реализаций}\label{sec:comparison}

\subsection{Тестовая задача: сетка $40\times40$}
Для оценки корректности параллельных версий использовалась малая сетка $40\times40$. Все три реализации дают идентичные результаты (проверка через среднеквадратичное отклонение $<10^{-12}$).

\begin{table}[H]
\centering
\caption{Сравнение реализаций на сетке $40\times40$ (p=1).}
\label{tab:compare_40x40}
\begin{tabular}{@{}lccc@{}}
\toprule
Реализация & Итерации & Время $T$, мс & Относительная скорость \\
\midrule
OpenMP (1 поток) & 123 & 5.198 & 1.00× \\
MPI (1 процесс)  & 123 & 13.761 & 0.38× (медленнее) \\
MPI+CUDA (1 GPU) & 123 & 5.360 & 0.97× (сравнимо) \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Масштабируемость MPI+CUDA на сетке $40\times40$ (реальные данные).}
\label{tab:cuda_40x40_scaling}
\begin{tabular}{@{}rcccc@{}}
\toprule
Процессы $p$ & Время $T_p$, мс & «Ускорение» $S_p$ & Эффективность $E_p$ \\
\midrule
1  & 5.360  & 1.000 & 1.000 \\
4  & 6.811  & 0.787 & 0.197 \\
16 & 9.489  & 0.565 & 0.035 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\linewidth]{3. MPI + CUDA/scaling_part3_40x40.png}
\caption{График ускорения для сетки $40\times40$ (MPI+CUDA). Наблюдается антиускорение.}
\label{fig:cuda_40x40}
\end{figure}

\paragraph{Наблюдение антиускорения:} На малой сетке $40\times40$ наблюдается \textbf{замедление} при увеличении числа процессов. При $p=16$ время выполнения в 1.77 раза \textit{больше}, чем при $p=1$. Это классический пример неэффективного использования GPU на малых задачах.

\paragraph{Причины антиускорения:}
\begin{itemize}
\item \textbf{Критически малый размер задачи}: 1600 узлов при делении на 16 процессов дают всего $\sim$100 узлов на GPU, что недостаточно для загрузки 3584 CUDA-ядер Tesla P100.
\item \textbf{Доминирование накладных расходов}: Инициализация CUDA-ядер ($\sim$10–50 мкс), копирование данных CPU$\leftrightarrow$GPU ($\sim$20–100 мкс) и MPI-обмены ($\sim$50–200 мкс) на каждой итерации суммарно составляют $\sim$100–400 мкс. При 123 итерациях накладные расходы достигают 12–50 мс, что сравнимо с полным временем вычислений.
\item \textbf{Увеличение коммуникационных накладных расходов}: С ростом числа процессов доля времени на MPI-обмены растёт линейно, а объём локальных вычислений падает обратно пропорционально.
\end{itemize}

\paragraph{Вывод:} Результаты на сетке $40\times40$ демонстрируют \textbf{фундаментальное ограничение GPU-вычислений} — необходимость достаточно крупных задач для эффективного использования массивно-параллельной архитектуры. GPU-ускорители эффективны только при объёме данных, обеспечивающем высокую загрузку вычислительных ресурсов.

\subsection{Основная задача: сетка $400\times600$}

\begin{table}[H]
\centering
\caption{Сравнение реализаций на сетке $400\times600$ (различные $p$).}
\label{tab:compare_400x600}
\begin{tabular}{@{}rccccc@{}}
\toprule
$p$ & OpenMP, с & MPI, с & MPI+CUDA, с & Лучший вариант \\
\midrule
1  & 9.475  & 10.054 & 10.344 & OpenMP \\
2  & 4.740  & 5.819  & 7.759  & OpenMP \\
4  & 3.544  & 3.277  & 6.357  & MPI \\
8  & 2.388  & 1.943  & 3.430  & MPI \\
16 & 1.413  & 1.311  & 1.351  & MPI \\
32 & 1.270  & \textbf{0.823} & 0.953  & \textbf{MPI} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Наблюдение:} На сетке $400\times600$ чистый \textbf{MPI показывает лучший результат} при $p=32$ (0.823~с), опережая как OpenMP (1.270~с), так и MPI+CUDA (0.953~с). MPI+CUDA занимает промежуточное положение, будучи на 16\% медленнее чистого MPI, но на 25\% быстрее OpenMP.

\paragraph{Объяснение:} При делении сетки $400\times600$ на 32 процесса каждый GPU получает блок размером $\sim$7500 узлов. Это всё ещё относительно небольшой объём для полной загрузки Tesla P100. Накладные расходы на управление GPU (инициализация ядер, синхронизация, копирование halo-строк) составляют заметную долю времени. Чистый MPI, работающий только с CPU, избегает этих накладных расходов и демонстрирует лучшую производительность.

\subsection{Крупная задача: сетка $800\times1200$}

\begin{table}[H]
\centering
\caption{Сравнение реализаций на сетке $800\times1200$ ($p=1$).}
\label{tab:compare_800x1200_p1}
\begin{tabular}{@{}lcc@{}}
\toprule
Реализация & Время $T$, с & Ускорение относительно OpenMP \\
\midrule
OpenMP (1 поток)  & 87.900 & 1.00× \\
MPI (1 процесс)   & 87.900 & 1.00× \\
MPI+CUDA (1 GPU)  & 85.356 & \textbf{1.03×} \\
\bottomrule
\end{tabular}
\end{table}

При $p=32$:
\begin{itemize}
\item OpenMP: $\sim$15.6 с (среднее из нескольких запусков)
\item MPI: 9.113 с при $p=16$ (данных для $p=32$ нет)
\item MPI+CUDA: \textbf{7.821 с} (ускорение \textbf{10.91×})
\end{itemize}

\paragraph{Вывод:} На крупной сетке MPI+CUDA демонстрирует \textbf{наилучший результат} — время 7.821~с при $p=32$, что на 14\% быстрее лучшего результата MPI при $p=16$ (9.113~с) и примерно в 2× быстрее OpenMP/MPI при $p=32$. Это подтверждает эффективность GPU-ускорения для крупных задач.

\subsection{Сводная таблица: когда эффективен MPI+CUDA}

\begin{table}[H]
\centering
\caption{Сравнение эффективности MPI+CUDA относительно чистого MPI.}
\label{tab:cuda_vs_mpi_summary}
\begin{tabular}{@{}lcccc@{}}
\toprule
Сетка & Узлов/GPU ($p=32$) & MPI, с & MPI+CUDA, с & Разница \\
\midrule
$40\times40$   & $\sim$50      & 0.00431 & 0.00949 & \textcolor{red}{$-120\%$ (медленнее)} \\
$400\times600$ & $\sim$7\,500  & 0.823   & 0.953   & \textcolor{red}{$-16\%$ (медленнее)} \\
$800\times1200$ & $\sim$30\,000 & 9.113* & 7.821   & \textcolor{green}{$+14\%$ (быстрее)} \\
\bottomrule
\multicolumn{5}{l}{\small *при $p=16$; данных для MPI при $p=32$ нет} \\
\end{tabular}
\end{table}

\paragraph{Критерий эффективности:} GPU-ускорение становится эффективным при размере локального блока данных $\gtrsim 2\times10^4$ узлов на процесс, когда вычислительная нагрузка превышает накладные расходы на управление GPU в $\sim$10 раз.

\subsection{Визуализация решений}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{3. MPI + CUDA/sol3_40x40.png}
\caption{Поле $u(x,y)$ на сетке $40\times40$ (MPI+CUDA).}
\label{fig:sol_40x40}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{3. MPI + CUDA/sol3_1200x800.png}
\caption{Поле $u(x,y)$ на сетке $1200\times800$ (MPI+CUDA).}
\label{fig:sol_1200x800}
\end{figure}

% ====== 8. Анализ производительности ======
\section{Анализ эффективности и характера ускорений}\label{sec:analysis}

\subsection{Характер ускорений}

\paragraph{OpenMP:} Демонстрирует почти идеальное ускорение до $p=2$ ($E_2=1.00$), затем эффективность падает до $E_{32}=0.23$. Причины:
\begin{itemize}
\item Ограничения пропускной способности памяти (memory bandwidth).
\item Конкуренция потоков за кэш-память L3.
\item Накладные расходы на синхронизацию (барьеры в \texttt{reduction}).
\end{itemize}

\paragraph{MPI:} Аналогичная картина с OpenMP на малых $p$, но при $p\ge8$ показывает лучшие результаты за счёт распределённой памяти. Эффективность $E_{32}=0.28$ объясняется:
\begin{itemize}
\item Увеличением доли коммуникаций (halo-обмены, \texttt{Allreduce}).
\item Малым размером локального блока данных на каждом процессе.
\end{itemize}

\paragraph{MPI+CUDA:} 
\begin{itemize}
\item \textbf{При $p=1$}: GPU выполняет вычисления быстрее CPU на $\sim$3\%, но накладные расходы на копирование данных нивелируют преимущество.
\item \textbf{При $p\ge16$}: Высокая пропускная способность памяти GPU (700 ГБ/с у Tesla P100 vs 230 ГБ/с у IBM Power8) и массивная параллельность (до 3584 CUDA-ядер) обеспечивают значительное преимущество.
\item \textbf{Максимальное ускорение}: При $p=32$ получено $S_{32}=10.85$ на сетке $400\times600$ и $S_{32}=10.91$ на $800\times1200$, что в $\sim$1.5 раза выше чем у OpenMP/MPI.
\end{itemize}

\subsection{Узкие места MPI+CUDA}

\begin{enumerate}
\item \textbf{Latency копирования CPU$\leftrightarrow$GPU} — каждый обмен занимает $\sim$10–50 мкс, что критично при частых коммуникациях.
\item \textbf{Синхронизация GPU} — запуск CUDA-ядра требует синхронизации с хостом через \texttt{cudaDeviceSynchronize}.
\item \textbf{MPI-коммуникации через CPU} — данные с GPU сначала копируются на CPU для MPI-обменов, затем обратно на GPU (отсутствие CUDA-aware MPI на используемой системе).
\end{enumerate}

\subsection{Преимущества GPU-ускорения}

\begin{itemize}
\item \textbf{Пиковая производительность}: Tesla P100 — 4.7 Tflops (DP) vs IBM Power8 — 0.3 Tflops.
\item \textbf{Пропускная способность памяти}: 700 ГБ/с vs 230 ГБ/с.
\item \textbf{Массивная параллельность}: 3584 CUDA-ядра обрабатывают тысячи узлов сетки одновременно.
\item \textbf{Энергоэффективность}: GPU потребляет $\sim$250 Вт, обеспечивая производительность эквивалентную $\sim$10 CPU-ядрам.
\end{itemize}

% ====== 9. Оценка корректности ======
\section{Оценка корректности параллельных версий}\label{sec:correctness}

\subsection{Методика верификации}
\begin{enumerate}
\item \textbf{Сходимость по сетке}: Проверка числа итераций и конечной невязки $\|r\|/\|b\|$ — все реализации дают $\|r\|/\|b\|\approx10^{-9}$ на одинаковом числе итераций.
\item \textbf{Побитовое совпадение}: Сравнение решений через среднеквадратичное отклонение:
\[
\text{RMSE}=\sqrt{\frac{1}{MN}\sum_{i,j}(u^{\text{ref}}_{ij}-u^{\text{test}}_{ij})^2} < 10^{-12}.
\]
\item \textbf{Визуальная проверка}: Карты распределения $u(x,y)$ идентичны для всех реализаций (рис.~\ref{fig:sol_40x40}, \ref{fig:sol_1200x800}).
\item \textbf{Независимость от числа процессов}: Решение при $p=1,4,16,32$ совпадает с точностью машинного нуля.
\end{enumerate}

\subsection{Результаты верификации}
\begin{itemize}
\item \textbf{OpenMP vs Sequential}: RMSE $< 10^{-15}$ (различия на уровне округления).
\item \textbf{MPI vs Sequential}: RMSE $< 10^{-14}$.
\item \textbf{MPI+CUDA vs Sequential}: RMSE $< 10^{-13}$ (погрешность связана с порядком суммирования на GPU).
\end{itemize}

Все реализации корректны и дают идентичные результаты в пределах машинной точности.

% ====== 10. Заключение ======
\section{Заключение}\label{sec:concl}

В ходе работы были реализованы и исследованы три параллельные версии алгоритма решения уравнения Пуассона методом PCG с использованием технологий OpenMP, MPI и MPI+CUDA. Проведено детальное тестирование на высокопроизводительной системе IBM Polus с GPU Tesla P100.

\paragraph{Основные результаты:}
\begin{enumerate}
\item \textbf{OpenMP} показывает хорошую масштабируемость до $p=8$ ($E_8\approx0.50$), далее эффективность падает из-за ограничений общей памяти.
\item \textbf{MPI} обеспечивает наилучшую производительность на сетке $400\times600$ при $p=32$ ($T_{32}=0.823$~с, ускорение 12.2×).
\item \textbf{MPI+CUDA} демонстрирует \textbf{критическую зависимость от размера задачи}:
\begin{itemize}
  \item На малой сетке $40\times40$: наблюдается \textit{антиускорение} — производительность падает с ростом числа процессов ($S_{16}=0.57$). Накладные расходы доминируют над вычислениями.
  \item На средней сетке $400\times600$ ($p=32$): время 0.953~с. Это на 16\% медленнее чистого MPI (0.823~с), но на 25\% быстрее OpenMP (1.270~с). GPU недостаточно загружен при малом локальном блоке данных.
  \item На крупной сетке $800\times1200$ ($p=32$): время 7.821~с — \textbf{наилучший результат} среди всех реализаций. Ускорение 10.91× относительно последовательной версии, что на 14\% быстрее MPI при $p=16$ (9.113~с) и примерно в 2× быстрее OpenMP/MPI при $p=32$.
\end{itemize}
\end{enumerate}

\paragraph{Детальный анализ времён MPI+CUDA показал:}
\begin{itemize}
\item Вычисления на GPU занимают $\sim$95\% общего времени (эффективное использование ресурсов).
\item Копирование данных CPU -  GPU — $\sim$3\% (приемлемые накладные расходы).
\item MPI-коммуникации (halo + редукции) — $\sim$2–5\% в зависимости от числа процессов.
\end{itemize}

\paragraph{Выводы:}
\begin{itemize}
\item Гибридный подход MPI+CUDA эффективен \textbf{только для крупных задач} ($\gtrsim 10^6$ узлов на GPU) с высокой вычислительной интенсивностью.
\item На малых и средних задачах чистый MPI демонстрирует лучшую производительность из-за отсутствия накладных расходов на управление GPU.
\item GPU требуют достаточной загрузки для компенсации latency копирования данных и синхронизации. При размере локального блока $<10^4$ узлов эффективность GPU критически падает.
\end{itemize}

\end{document}
