\documentclass[12pt,a4paper]{article}

% --- Русская типографика и математика ---
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\geometry{left=25mm,right=15mm,top=20mm,bottom=20mm}
\usepackage{microtype}
\usepackage{float} 

% --- Графика, таблицы, ссылки ---
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\sisetup{output-decimal-marker={,},round-mode=places,round-precision=3}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue}

% --- Пакеты для графиков ---
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% --- Листинги ---
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  columns=fullflexible,
  tabsize=2,
  showstringspaces=false
}

% --- Удобные обозначения ---
\newcommand{\RR}{\mathbb{R}}
\newcommand{\grad}{\nabla}
\newcommand{\lap}{\Delta}
\newcommand{\dd}{\,\mathrm{d}}

\begin{document}
\thispagestyle{empty}

\begin{center}
\vspace{-3cm}

\includegraphics[width=0.5\textwidth]{msu.eps}\\
{\scshape Московский государственный университет имени М.~В.~Ломоносова}\\
Факультет вычислительной математики и кибернетики\\
Кафедра системного анализа

\vfill

{\LARGE Отчет по заданию}

\vspace{1cm}

{\Huge\bfseries <<Реализация параллельного алгоритма с использованием технологий MPI и OpenMP>>}
\end{center}

\vspace{1cm}

\begin{flushright}
  \large
  \textit{Студент 616 группы}\\
  М.~Н.~Преображенский\\

  \vspace{5mm}

\end{flushright}

\vfill

\begin{center}
{\Large 14 декабря 2025}
\end{center}
\newpage
\tableofcontents
\newpage

% ====== 1. Введение ======
\section{Введение}
Цель работы — решить двумерную задачу Дирихле для уравнения Пуассона в криволинейной области методом фиктивных областей, реализовать и сравнить три варианта распараллеливания:
\begin{itemize}
  \item OpenMP (многопоточность в рамках одного узла);
  \item MPI (распределённые вычисления между процессами);
  \item MPI+OpenMP (гибридный подход).
\end{itemize}
Исследуется масштабируемость каждого подхода на ПВС IBM~Polus.

% ====== 2. Математическая постановка задачи ======
\section{Математическая постановка задачи}\label{sec:math}
Рассматривается задача Пуассона в криволинейной области $D\subset\RR^2$, ограниченной контуром $\gamma$:
\begin{equation}\label{eq:poisson}
  -\lap u = f(x,y), \quad (x,y)\in D,
\end{equation}
с граничным условием Дирихле первого рода
\begin{equation}\label{eq:dirichlet}
  u(x,y)=0, \quad (x,y)\in\gamma.
\end{equation}
В данной работе $f(x,y)\equiv1$. Для \textbf{варианта 10} область $D$ задаётся неравенствами
\[
  D=\{(x,y):\ x^2-4y^2>1,\ 1<x<3\},
\]
то есть область ограничена дугой гиперболы и отрезком прямой $x=3$.

% ====== 3. Численный метод ======
\section{Краткое описание численного метода решения}\label{sec:numerics}

\subsection{Метод фиктивных областей}
Пусть $D\subset\Pi=\{(x,y): A_1<x<B_1,\ A_2<y<B_2\}$ — охватывающий прямоугольник, $\widehat D=\Pi\setminus D$ — фиктивная область. В~$\Pi$ решается задача
\begin{equation}\label{eq:mfoPDE}
 -\frac{\partial}{\partial x}\!\Big(k\,u_x\Big) - \frac{\partial}{\partial y}\!\Big(k\,u_y\Big)=F(x,y),\quad (x,y)\in \Pi\setminus\gamma,\qquad u|_{\partial\Pi}=0,
\end{equation}
где кусочно-постоянный коэффициент
\[
k(x,y)=
\begin{cases}
1, & (x,y)\in D,\\
1/\varepsilon, & (x,y)\in \widehat D,
\end{cases}
\qquad \varepsilon=\max(h_x,h_y)^2.
\]

\subsection{Разностная схема}
Покроем $\Pi$ равномерной сеткой $\omega_h$ с внутренними узлами $M\times N$, шаги $h_x=\frac{B_1-A_1}{M+1}$, $h_y=\frac{B_2-A_2}{N+1}$. Дифференциальный оператор аппроксимируем пятиточечным шаблоном:
\begin{align}\label{eq:divscheme}
 -\frac{1}{h_x}\!\left(a_{i+1,j}\,\frac{w_{i+1,j}-w_{i,j}}{h_x}-a_{i,j}\,\frac{w_{i,j}-w_{i-1,j}}{h_x}\right)
 -\frac{1}{h_y}\!\left(b_{i,j+1}\,\frac{w_{i,j+1}-w_{i,j}}{h_y}-b_{i,j}\,\frac{w_{i,j}-w_{i,j-1}}{h_y}\right)
 = F_{ij},
\end{align}
где \emph{гранные коэффициенты} вычисляются аналитически через длины пересечений граней с областью $D$.

\subsection{Итерационный метод}
Для решения результирующей СЛАУ $Aw=F$ применяется \textbf{предобусловленный метод сопряжённых градиентов (PCG)} с диагональным предобуславливателем Якоби. Критерий остановки: $\|r\|/\|b\|\le10^{-8}$ или достижение максимального числа итераций.

\paragraph{Базовые условия для сравнения производительности.}
Во всех сериях измерений число итераций PCG одинаково для заданного размера сетки, поэтому сравнение по времени корректно:
\begin{itemize}
  \item $400{\times}600$: \textbf{1384} итерации;
  \item $800{\times}1200$: \textbf{2599} итераций.
\end{itemize}

% ====== 4. OpenMP реализация ======
\section{OpenMP-реализация}\label{sec:omp}

\subsection{Описание решения OpenMP}
В OpenMP-версии используется многопоточное распараллеливание циклов в рамках одного процесса:

\begin{itemize}
  \item \textbf{Вычисление коэффициентов.} Циклы по узлам сетки для расчёта $a_{i,j}$, $b_{i,j}$, $F_{ij}$ и диагонали $A_{\text{diag}}$ распараллеливаются директивой \texttt{\#pragma omp parallel for collapse(2) schedule(static)}.
  
  \item \textbf{Матрично-векторное произведение.} Применение пятиточечного шаблона к вектору выполняется параллельно по всем внутренним узлам сетки.
  
  \item \textbf{Скалярное произведение.} Используется директива \texttt{reduction(+:sum)} для корректного суммирования частичных результатов из разных потоков.
  
  \item \textbf{Векторные операции (axpy).} Обновление векторов $u$, $r$, $p$ выполняется параллельно с балансировкой нагрузки через \texttt{schedule(static)}.
\end{itemize}

\paragraph{Ключевые особенности реализации:}
\begin{itemize}
  \item Использование \texttt{collapse(2)} для двумерных циклов позволяет увеличить гранулярность параллелизма.
  \item Статическое распределение итераций (\texttt{schedule(static)}) обеспечивает минимальные накладные расходы и предсказуемое распределение нагрузки.
  \item Отсутствие false sharing за счёт работы с непересекающимися участками памяти.
\end{itemize}

\subsection{Анализ улучшения OpenMP}\label{sec:omp_analysis}

\begin{table}[H]
\centering
\caption{OpenMP: strong scaling на сетке $400{\times}600$ (IBM~Polus). База: 2 потока.}
\label{tab:omp_400x600}
\begin{tabular}{@{}rccc@{}}
\toprule
Потоков $t$ & Итерации & Время $T_t$, c & Ускорение $S_t$ \\
\midrule
2  & 1384 & 6.91 & 1.00 \\
16 & 1384 & 2.58 & 2.67 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{OpenMP: strong scaling на сетке $800{\times}1200$ (IBM~Polus). База: 4 потока.}
\label{tab:omp_800x1200}
\begin{tabular}{@{}rccc@{}}
\toprule
Потоков $t$ & Итерации & Время $T_t$, c & Ускорение $S_t$ \\
\midrule
4  & 2599 & 41.24 & 1.00 \\
32 & 2599 & 24.69 & 1.67 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Выводы по OpenMP.}
OpenMP демонстрирует \textbf{ограниченную масштабируемость}. Для сетки $400{\times}600$ получено максимальное ускорение \textbf{2.67$\,\times$} (при 16 потоках), а для более крупной сетки $800{\times}1200$ — лишь \textbf{1.67$\,\times$} (при 32 потоках). Невысокая масштабируемость объясняется:
\begin{itemize}
  \item Накладными расходами на синхронизацию (барьеры, редукции).
  \item Конкуренцией за пропускную способность памяти (memory bandwidth).
  \item NUMA-эффектами при доступе к удалённой памяти.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{scaling_400x600.png}
\caption{График ускорения $S_t$ для сетки $400{\times}600$ (OpenMP, strong scaling).}
\label{fig:omp_scaling_400x600}
\end{figure}

% ====== 5. MPI реализация ======
\section{MPI-реализация}\label{sec:mpi}

\subsection{Описание решения MPI}
В MPI-версии используется двумерная декомпозиция области (2D decomposition) с Cartesian-коммуникатором:

\begin{itemize}
  \item \textbf{Распределение данных.} Сетка $M\times N$ разбивается на $p_x \times p_y$ блоков, где $p_x \cdot p_y = p$ — общее число MPI-процессов. Каждый процесс хранит локальный блок размера $\texttt{local\_M} \times \texttt{local\_N}$ с ghost-слоями толщиной 1.
  
  \item \textbf{Декартов коммуникатор.} Создаётся с помощью \texttt{MPI\_Cart\_create}, что позволяет использовать \texttt{MPI\_Cart\_shift} для определения соседей по четырём направлениям (north, south, west, east).
  
  \item \textbf{Halo-обмены.} Перед каждым матрично-векторным произведением выполняется асинхронный обмен граничными строками/столбцами с соседями через \texttt{MPI\_Isend}/\texttt{MPI\_Irecv}.
  
  \item \textbf{Глобальные редукции.} Скалярные произведения вычисляются локально, затем суммируются через \texttt{MPI\_Allreduce}.
  
  \item \textbf{Сбор решения.} Результат собирается на процессе 0 через \texttt{MPI\_Gatherv}.
\end{itemize}

\paragraph{Особенности 2D-декомпозиции:}
Оптимальное разбиение $p_x \times p_y$ выбирается так, чтобы локальные домены были близки к квадратным, минимизируя отношение периметра к площади и, следовательно, объём коммуникаций.

\subsection{Анализ улучшения MPI}\label{sec:mpi_analysis}

\begin{table}[H]
\centering
\caption{MPI: strong scaling на сетке $400{\times}600$ (IBM~Polus). База: 2 процесса.}
\label{tab:mpi_400x600}
\begin{tabular}{@{}rccc@{}}
\toprule
$p$ & Итерации & Время $T_p$, c & Ускорение $S_p$ \\
\midrule
2  & 1384 & 22.45 & 1.00 \\
16 & 1384 & 1.72  & 13.05 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{MPI: strong scaling на сетке $800{\times}1200$ (IBM~Polus). База: 4 процесса.}
\label{tab:mpi_800x1200}
\begin{tabular}{@{}rccc@{}}
\toprule
$p$ & Итерации & Время $T_p$, c & Ускорение $S_p$ \\
\midrule
4  & 2599 & 18.79 & 1.00 \\
16 & 2599 & 4.35  & 4.31 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Выводы по MPI.}
MPI демонстрирует лучшую масштабируемость среди рассматриваемых подходов. На сетке $400{\times}600$ ускорение достигает \textbf{13.05$\,\times$} (при 16 процессах относительно базы $p=2$), а на сетке $800{\times}1200$ — \textbf{4.31$\,\times$} (при 16 процессах относительно базы $p=4$). При увеличении числа процессов ускорение ограничивается из-за:
\begin{itemize}
  \item Роста объёма коммуникаций (halo-обмены) при уменьшении размера локальных блоков.
  \item Накладных расходов на глобальные редукции (\texttt{MPI\_Allreduce}).
  \item Несбалансированности нагрузки при неравномерном распределении области $D$ между процессами.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{2_scaling_800x1200.png}
\caption{График ускорения $S_p$ для сетки $800{\times}1200$ (MPI, strong scaling).}
\label{fig:mpi_scaling_800x1200}
\end{figure}

% ====== 6. MPI+OpenMP реализация ======
\section{Гибридная MPI+OpenMP реализация}\label{sec:hybrid}

\subsection{Описание решения MPI+OpenMP}
Гибридная реализация комбинирует MPI для межузловых коммуникаций и OpenMP для внутриузлового параллелизма:

\begin{itemize}
  \item \textbf{MPI-уровень.} Используется 2D-декомпозиция области с Cartesian-коммуникатором. Инициализация MPI выполняется с уровнем потокобезопасности \texttt{MPI\_THREAD\_FUNNELED}, что гарантирует корректность при вызове MPI-функций из главного потока.
  
  \item \textbf{OpenMP-уровень.} Внутри каждого MPI-процесса вычислительно ёмкие циклы распараллеливаются с помощью OpenMP:
  \begin{itemize}
    \item Вычисление коэффициентов $a_{i,j}$, $b_{i,j}$, $F_{ij}$ — \texttt{\#pragma omp parallel for collapse(2)}.
    \item Матрично-векторное произведение — параллельный обход локальных узлов.
    \item Скалярное произведение — редукция \texttt{reduction(+:local\_sum)}, после чего результат передаётся в \texttt{MPI\_Allreduce}.
    \item Векторные обновления PCG — параллельные циклы.
  \end{itemize}
  
  \item \textbf{Схема взаимодействия.}
  \begin{enumerate}
    \item OpenMP-потоки параллельно вычисляют локальную часть matvec.
    \item Главный поток (или все потоки с барьером) упаковывает граничные данные.
    \item MPI-обмен halo-данными между соседними процессами.
    \item OpenMP-потоки продолжают итерацию PCG.
  \end{enumerate}
\end{itemize}

\paragraph{Преимущества гибридного подхода:}
\begin{itemize}
  \item Уменьшение числа MPI-процессов при том же уровне параллелизма снижает объём коммуникаций.
  \item Более эффективное использование общей памяти внутри NUMA-узла.
  \item Лучшая масштабируемость на системах с многоядерными узлами.
\end{itemize}

\subsection{Анализ улучшения MPI+OpenMP}\label{sec:hybrid_analysis}

\paragraph{Сетка $400{\times}600$ (фиксировано $p=2$ MPI-процесса).}
\begin{table}[H]
\centering
\caption{MPI+OpenMP на $400{\times}600$ (2 MPI-процесса). База: 1 поток/процесс.}
\label{tab:hybrid_400x600}
\begin{tabular}{@{}rcc@{}}
\toprule
Потоков на процесс & Время $T$, c & Ускорение $S$ \\
\midrule
1 & 15.94 & 1.00 \\
4 & 3.64  & 4.38 \\
8 & 1.77  & 9.00 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Сетка $800{\times}1200$ (фиксировано $p=4$ MPI-процесса).}
\begin{table}[H]
\centering
\caption{MPI+OpenMP на $800{\times}1200$ (4 MPI-процесса). База: 1 поток/процесс.}
\label{tab:hybrid_800x1200}
\begin{tabular}{@{}rcc@{}}
\toprule
Потоков на процесс & Время $T$, c & Ускорение $S$ \\
\midrule
1 & 13.24 & 1.00 \\
4 & 7.41  & 1.78 \\
8 & 5.51  & 2.40 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Выводы по MPI+OpenMP.}
Гибридный подход показывает высокие ускорения на сетке $400{\times}600$ (до \textbf{9$\,\times$} при 8 потоках на процесс), однако на более крупной задаче $800{\times}1200$ эффективность ограничена коммуникациями и синхронизациями (ускорение до \textbf{2.40$\,\times$}).

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{scaling_800x1200.png}
\caption{Сравнение ускорений для различных режимов распараллеливания на сетке $800{\times}1200$.}
\label{fig:hybrid_comparison}
\end{figure}

\paragraph{Сетка $1200{\times}800$ (фиксировано $p=4$ MPI-процесса).}
\begin{table}[H]
\centering
\caption{MPI+OpenMP на $1200{\times}800$ (4 MPI-процесса). База: 1 поток/процесс.}
\label{tab:hybrid_1200x800}
\begin{tabular}{@{}rcc@{}}
\toprule
Потоков на процесс & Время $T$, c & Ускорение $S$ \\
\midrule
1 & 11.84 & 1.00 \\
2 & 7.28  & 1.63 \\
4 & 6.12  & 1.93 \\
8 & 4.89  & 2.42 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{3_scaling_1200x800.png}
\caption{График ускорения $S$ для сетки $1200{\times}800$ (MPI+OpenMP, 4 MPI-процесса, strong scaling по потокам).}
\label{fig:hybrid_scaling_1200x800}
\end{figure}

% ====== 7. Сводное сравнение ======
\section{Сводное сравнение реализаций}\label{sec:comparison}

\begin{table}[H]
\centering
\caption{Сравнение максимальных ускорений (IBM~Polus).}
\label{tab:comparison_summary}
\begin{tabular}{@{}lcc@{}}
\toprule
Реализация & $400{\times}600$ & $800{\times}1200$ \\
\midrule
OpenMP & 2.67$\,\times$ & 1.67$\,\times$ \\
MPI & \textbf{13.05$\,\times$} & \textbf{4.31$\,\times$} \\
MPI+OpenMP & 9.00$\,\times$ & 2.40$\,\times$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Ключевые наблюдения.}
MPI демонстрирует наилучшую масштабируемость и максимальные ускорения (особенно на $400{\times}600$). OpenMP ограничен архитектурой общей памяти и даёт умеренный выигрыш. Гибридный MPI+OpenMP эффективен на меньшей сетке, но на большой задаче уступает чистому MPI из-за роста коммуникационных и синхронизационных накладных расходов.

% ====== 8. Визуализация решения ======
\section{Визуализация численного решения}\label{sec:visualization}

На рис.~\ref{fig:solution_40x40}--\ref{fig:solution_1200x800} представлены карты распределения потенциала $u(x,y)$ на сетках различного разрешения. Решение корректно воспроизводит граничные условия (нулевые значения на границе области $D$) и физически обоснованный характер распределения.

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{sol2_40x40.png}
\caption{Поле распределения потенциала $u(x,y)$ для размера сетки $40{\times}40$.}
\label{fig:solution_40x40}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\linewidth]{sol2_1200x800.png}
\caption{Поле распределения потенциала $u(x,y)$ для размера сетки $1200{\times}800$.}
\label{fig:solution_1200x800}
\end{figure}

% ====== 10. Заключение ======
\section{Заключение}\label{sec:concl}

В ходе работы были реализованы и исследованы три варианта параллельного алгоритма решения уравнения Пуассона методом PCG: OpenMP, MPI и гибридный MPI+OpenMP. Проведено тестирование на IBM~Polus с различными конфигурациями.

\paragraph{Основные результаты:}
\begin{itemize}
  \item \textbf{MPI} демонстрирует наилучшую масштабируемость и максимальные ускорения: до \textbf{13.05$\,\times$} на сетке $400{\times}600$ и до \textbf{4.31$\,\times$} на $800{\times}1200$ (относительно указанных базовых конфигураций).
  \item \textbf{OpenMP} ограничен архитектурой общей памяти и даёт умеренный прирост: до \textbf{2.67$\,\times$} (на $400{\times}600$) и до \textbf{1.67$\,\times$} (на $800{\times}1200$).
  \item \textbf{MPI+OpenMP} показывает промежуточные результаты: до \textbf{9.00$\,\times$} на $400{\times}600$ и до \textbf{2.40$\,\times$} на $800{\times}1200$; на большой задаче гибрид уступает чистому MPI из-за коммуникаций.
\end{itemize}

\paragraph{Рекомендации:}
\begin{itemize}
  \item Если доступна распределённая среда и важна производительность, \textbf{предпочтителен чистый MPI}.
  \item Гибридный MPI+OpenMP имеет смысл при ограничении числа MPI-процессов на узел и для более мелких сеток, где он даёт существенный выигрыш.
  \item Чистый OpenMP целесообразен для простоты запуска на одном узле, но его масштабируемость ограничена.
\end{itemize}

\end{document}
